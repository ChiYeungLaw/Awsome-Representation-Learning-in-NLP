# Awsome-Representation-Learning-in-NLP
This repo contains some research about Representation Learning in NLP. I summarize some papers and categorize them by myself. You are kindly invited to pull requests.

This book [Representation Learning For Natural Language Processing](https://link.springer.com/chapter/10.1007%2F978-981-15-5573-2_2#citeas) has good coverge of this area.

## Word Representation

### Distributed Word Representation

1. (**Brown Cluster**) Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based <i>n</i>-gram models of natural language. <i>Comput. Linguist.</i> 18, 4 (December 1992), 467–479. [[paper]](https://dl.acm.org/doi/abs/10.5555/176313.176316)
2. (**LSA**) Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R. Indexing by latent semantic analysis. *Journal of the American Society for Information Science*. 1990;41:391. [[paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/JASIS90.pdf)
3. (**NNLM**) Bengio, Y. & Ducharme, Réjean & Vincent, Pascal. (2000). A Neural Probabilistic Language Model. Journal of Machine Learning Research. 3. 932-938. 10.1162/153244303322533223.  [[paper]](https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
4. (**The first work used unlabeled data to obtain word embeddings and used them in multi-tasks.**) Collobert, Ronan & Weston, Jason & Bottou, Leon & Karlen, Michael & Kavukcuoglu, Koray & Kuksa, Pavel. (2011). Natural Language Processing (Almost) from Scratch. Computing Research Repository - CORR. 12. [[paper]](https://arxiv.org/abs/1103.0398)
5. (**The first Word2Vec Paper**) Mikolov, Tomas & Sutskever, Ilya & Chen, Kai & Corrado, G.s & Dean, Jeffrey. (2013). Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems. 26. [[paper]](https://arxiv.org/abs/1310.4546)
6. (**Another Word2Vec Paper**) Mikolov, Tomas & Chen, Kai & Corrado, G.s & Dean, Jeffrey. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013. [[paper]](https://arxiv.org/abs/1301.3781)
7. (**Word2Vec Negative Sampling Explaination**) Goldberg, Yoav & Levy, Omer. (2014). word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method. [[paper]](https://arxiv.org/pdf/1402.3722v1.pdf)
8. (**Another Word2Vec Explaination Paper**) Rong, Xin. (2014). Word2vec Parameter Learning Explained. [[paper]](https://arxiv.org/abs/1411.2738) (Dr. Xin passed away in 2017. R.I.P)
9. (**GloVe**) Pennington, Jeffrey & Socher, Richard & Manning, Christoper. (2014). Glove: Global Vectors for Word Representation. EMNLP. 14. 1532-1543. 10.3115/v1/D14-1162. [[paper]](https://www.aclweb.org/anthology/D14-1162/)
10. (**Theoretical analysis of SGNS**) Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In *Proceedings of NeurIPS*, 2014. [[paper]](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization)

